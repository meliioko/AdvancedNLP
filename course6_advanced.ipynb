{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Course 6: Advanced NLP Tasks\n",
        "\n",
        "We'll continues pre-training a language model on conll03 before adding a CRF on top of it in order to perform NER.\n"
      ],
      "metadata": {
        "id": "ypuFbq45Rdli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data and Preprocessing\n",
        "\n",
        "We'll need to download several data files:\n",
        "* [The language model's training corpus](https://github.com/Madjakul/MiNER/blob/main/data/conll/conll_train_corpus.txt)\n",
        "* [The language model's validation corpus](https://github.com/Madjakul/MiNER/blob/main/data/conll/conll_dev_corpus.txt)\n",
        "* [The list of labels](https://github.com/Madjakul/MiNER/blob/main/data/conll/labels.txt)\n",
        "* [The NER's training corpus](https://github.com/Madjakul/MiNER/blob/main/data/conll/gold/conll_train.conll)\n",
        "* [The NER's validation corpus](https://github.com/Madjakul/MiNER/blob/main/data/conll/gold/conll_dev.conll)\n",
        "* [The NER's test corpus](https://github.com/Madjakul/MiNER/blob/main/data/conll/gold/conll_test.conll)"
      ],
      "metadata": {
        "id": "NA3SSuh5Sq-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[torch] accelerate seqeval"
      ],
      "metadata": {
        "id": "riLhv_J4UCKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    torch.cuda.empty_cache()\n",
        "else: DEVICE = \"cpu\"\n",
        "\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "Q2jLJLXTVTpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "\n",
        "class RoBERTa():\n",
        "    \"\"\"Transformer model for short english sentences. Based on RoBERTa [1]_.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    device: str,  {\"cuda\", \"cpu\"}\n",
        "        The hardware that will perform the computations.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    model: transformers.RobertaForMaskedLM\n",
        "        Transformer model for masked language modeling.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    ..  [1] Yinhan Liu et al. 2019. Roberta: A robustly optimized Bert\n",
        "        pretraining approach. (July 2019). Retrieved January 31, 2023 from\n",
        "        https://arxiv.org/abs/1907.11692\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device: Literal[\"cuda\", \"cpu\"]):\n",
        "        self.model = RobertaForMaskedLM.from_pretrained(\n",
        "            \"roberta-base\"\n",
        "        ).to(device)    # type: ignore"
      ],
      "metadata": {
        "id": "-p93RBYbUdPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the `add_vocab` function, why do we proceed this way?"
      ],
      "metadata": {
        "id": "DgYf9VXOW5JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "import datasets\n",
        "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "\n",
        "class TransformerDataset():\n",
        "    \"\"\"Custom dataset used to pretrain Transformers checkpoints from\n",
        "    **HuggingFace**.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    train_corpus: List[str]\n",
        "        List of training texts.\n",
        "    valid_corpus: List[str]\n",
        "        List of validation texts.\n",
        "    max_length: int\n",
        "        Maximum sequence length.\n",
        "    mlm_probability: float\n",
        "        Proportion of words to mask from the training and validation corpus.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    mlm_ds:\n",
        "        Maps the tokenising function to the **HuggingFace**'s ``datasets``.\n",
        "    max_length: int\n",
        "        Maximum sequence length.\n",
        "    train_corpus: list\n",
        "        List of training sentences.\n",
        "    valid_corpus: List[str]\n",
        "        List of validation sentences.\n",
        "    tokenizer: transformers.AutoTokenizer\n",
        "        Object from ``AutoTokenizer``. The object depends on the language\n",
        "        model used.\n",
        "    data_collator: transformers.DataCollatorForLanguageModeling\n",
        "        Data collator to mask a given proportion of word from the corpus before\n",
        "        returning a tokenized and encoded version of it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, train_corpus: List[str], valid_corpus: List[str],\n",
        "        max_length: int, mlm_probability: float\n",
        "    ):\n",
        "        self.mlm_ds = None\n",
        "        self.max_length = max_length\n",
        "        self.train_corpus = train_corpus\n",
        "        self.valid_corpus = valid_corpus\n",
        "        print(f\"Using roberta-base tokenizer\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"roberta-base\",\n",
        "        )\n",
        "        self._build_mlm_dataset()\n",
        "        self.data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm_probability=mlm_probability,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def _build_mlm_dataset(self):\n",
        "        train_ds = {\"text\": self.train_corpus}\n",
        "        valid_ds = {\"text\": self.valid_corpus}\n",
        "        ds = datasets.DatasetDict({\n",
        "            \"train\": datasets.Dataset.from_dict(train_ds),\n",
        "            \"valid\": datasets.Dataset.from_dict(valid_ds)\n",
        "        })\n",
        "        self.mlm_ds = ds.map(self._tokenize, batched=True)\n",
        "        self.mlm_ds.remove_columns([\"text\"])\n",
        "\n",
        "    def _tokenize(self, batch):\n",
        "        return self.tokenizer(\n",
        "            batch[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_special_tokens_mask=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def add_vocab( self, corpus: List[str], lm: RoBERTa):\n",
        "        \"\"\"Adds new tokens to a pretrained LLM. The embedding of the added\n",
        "        tokens are initialized using the mean of the already existing tokens\n",
        "        plus some noise in order to avoid diverging too much from the initial\n",
        "        distributions, thus converging faster during pretraining [1]_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        corpus: ``list``\n",
        "            List of tokens per document.\n",
        "        lm: miner.modules.RoBERTa\n",
        "            Pretrained large language model.\n",
        "\n",
        "        References\n",
        "        ----------\n",
        "        ..  [1] Hewitt John. 2021. Initializing new word embeddings for\n",
        "            pretrained language models. (2021). Retrieved April 24, 2023 from\n",
        "            https://nlp.stanford.edu/~johnhew/vocab-expansion.html\n",
        "        \"\"\"\n",
        "        new_tokens = [\n",
        "            token for text in corpus for token in text.split()\n",
        "        ]\n",
        "        new_tokens = set(new_tokens) - set(self.tokenizer.vocab.keys()) # New tokens don't already exist\n",
        "        print( f\"Adding {len(new_tokens)} new tokens to the vocabulary\")\n",
        "        self.tokenizer.add_tokens(list(new_tokens))\n",
        "        print(\"Resizing the Language model\")\n",
        "        lm.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        # Computing the distribution of the new embeddings\n",
        "        params = lm.model.state_dict()\n",
        "        # embeddings = params[\"transformer.wte.weight\"]\n",
        "        embeddings_key = \"roberta.embeddings.word_embeddings.weight\"\n",
        "        embeddings = params[embeddings_key]\n",
        "        pre_expansion_embeddings = embeddings[:-3, :]\n",
        "        mu = torch.mean(pre_expansion_embeddings, dim=0)\n",
        "        n = pre_expansion_embeddings.size()[0]\n",
        "        sigma = (\n",
        "            (pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)\n",
        "        ) / n\n",
        "        dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
        "            mu,\n",
        "            covariance_matrix=1e-5*sigma\n",
        "        )\n",
        "        # Loading the new embeddings in the model\n",
        "        new_embeddings = torch.stack(\n",
        "            tuple((dist.sample() for _ in range(3))),\n",
        "            dim=0\n",
        "        )\n",
        "        embeddings[-3:, :] = new_embeddings\n",
        "        params[embeddings_key][-3:, :] = new_embeddings\n",
        "        lm.model.load_state_dict(params)"
      ],
      "metadata": {
        "id": "-wjUpkf4UElf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's the relation between batch size and the gradient accumulation?"
      ],
      "metadata": {
        "id": "DnMYAvF_X6yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "\n",
        "class TransformerTrainer():\n",
        "    \"\"\"Wrapper for the transformers ``Trainer`` class to perform domain-\n",
        "    specific MLM [1]_ before adding the NER head [2]_.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lm: miner.modules.RoBERTa\n",
        "        Language model checkpoint from **HuggingFace**.\n",
        "    lm_path: str\n",
        "        Path to the local file that will contained the trained language model.\n",
        "    lm_dataset: miner.utils.data.TransformerDataset\n",
        "        Iterable object containing the training and validation data.\n",
        "    per_device_train_batch_size: int\n",
        "        Training batch size.\n",
        "    seed: int\n",
        "        Integers used to initialized the weight of the LLM. Used for\n",
        "        replicability.\n",
        "    per_device_eval_batch_size: int\n",
        "        Validation batch size.\n",
        "    num_train_epochs: int\n",
        "        Maximum number of training epochs.\n",
        "    gradient_accumulation_steps: int\n",
        "        For how manys steps the gradient is accumulated.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    training_args: transformers.TrainingArguments\n",
        "        Stores the hyperparameters to pretrain the large language model.\n",
        "    trainer: transformers.Trainer\n",
        "        Stores the datasets to perform MLM and feed the large language model\n",
        "        with.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    ..  [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\n",
        "        2019. Bert: Pre-training of deep bidirectional Transformers for\n",
        "        language understanding. (May 2019). Retrieved January 31, 2023 from\n",
        "        https://arxiv.org/abs/1810.04805v2\n",
        "    ..  [2] Suchin Gururangan et al. 2020. Don't stop pretraining: Adapt\n",
        "        language models to domains and tasks. (May 2020). Retrieved January 31,\n",
        "        2023 from https://arxiv.org/abs/2004.10964\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, lm: RoBERTa, lm_path: str,\n",
        "        lm_dataset: TransformerDataset, per_device_train_batch_size: int,\n",
        "        seed: int, per_device_eval_batch_size: int, max_steps: int,\n",
        "        gradient_accumulation_steps: int, wandb: bool\n",
        "    ):\n",
        "        self.lm_path = lm_path\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=lm_path,\n",
        "            overwrite_output_dir=True,\n",
        "            do_train=True,\n",
        "            do_eval=True,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            eval_accumulation_steps=gradient_accumulation_steps,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "            max_steps=max_steps,\n",
        "            num_train_epochs=max_steps/ (\n",
        "                len(lm_dataset.mlm_ds[\"train\"]) / per_device_train_batch_size\n",
        "            ),\n",
        "            logging_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            seed=seed,\n",
        "            data_seed=seed,\n",
        "            log_level=\"error\",\n",
        "            report_to=\"wandb\" if wandb else \"none\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "            save_total_limit=1\n",
        "        )\n",
        "        self.trainer = Trainer(\n",
        "            model=lm.model,\n",
        "            args=self.training_args,\n",
        "            data_collator=lm_dataset.data_collator,\n",
        "            train_dataset=lm_dataset.mlm_ds[\"train\"],   # type: ignore\n",
        "            eval_dataset=lm_dataset.mlm_ds[\"valid\"]     # type: ignore\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Performs MLM to further pretrain a large language model.\n",
        "        \"\"\"\n",
        "        self.trainer.train()\n",
        "        self.trainer.save_model(self.lm_path)"
      ],
      "metadata": {
        "id": "JLyiTC6LWg5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the hyperparameter to pretrain the LM"
      ],
      "metadata": {
        "id": "k18SSPZ1ZHoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "    train_corpus_path = \"\" # Complete\n",
        "    val_corpus_path = \"\" # Complete\n",
        "    max_length = 256 # Can be modified\n",
        "    mlm_probability = 0.15 # Can be modified\n",
        "    lm_path = \"./lm\" # Can be modified\n",
        "    lm_train_batch_size =  # Complete\n",
        "    max_steps =  1 # Can be modified\n",
        "    lm_accumulation_steps =  # Complete\n",
        "\n",
        "    print(f\"Loading training data from {train_corpus_path}\")\n",
        "    with open(train_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        train_corpus = f.read().splitlines()\n",
        "    print(f\"Loading validation data from {val_corpus_path}\")\n",
        "    with open(val_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        val_corpus = f.read().splitlines()\n",
        "\n",
        "    print(\"Using  RoBERTa checkpoint as language model\")\n",
        "    lm = RoBERTa(DEVICE)\n",
        "\n",
        "    print(\"Building the dataset...\")\n",
        "    lm_dataset = TransformerDataset(\n",
        "        train_corpus=train_corpus,\n",
        "        valid_corpus=val_corpus,\n",
        "        max_length=max_length,\n",
        "        mlm_probability=mlm_probability,\n",
        "    )\n",
        "\n",
        "    lm_dataset.add_vocab(train_corpus, lm)\n",
        "\n",
        "    print(\"*** Training ***\")\n",
        "    lm_trainer = TransformerTrainer(\n",
        "        lm=lm,\n",
        "        lm_path=lm_path,\n",
        "        lm_dataset=lm_dataset,\n",
        "        per_device_train_batch_size=lm_train_batch_size,\n",
        "        seed=0,\n",
        "        per_device_eval_batch_size=lm_train_batch_size,\n",
        "        max_steps=max_steps,\n",
        "        gradient_accumulation_steps=lm_accumulation_steps,\n",
        "        wandb=False\n",
        "    )\n",
        "    lm_trainer.train()\n",
        "    print(\"=== Done ===\")"
      ],
      "metadata": {
        "id": "MTS6cuEgUt-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRF"
      ],
      "metadata": {
        "id": "Z9AoWLjzfVr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNLABELED_INDEX = -1\n",
        "IMPOSSIBLE_SCORE = -100\n",
        "\n",
        "\n",
        "def create_possible_tag_masks(num_tags: int, tags: torch.LongTensor):\n",
        "    \"\"\"Creates a mask-like sparse tensor where the index of the correct tag has\n",
        "    a value of 1, allowing for multilabel targets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_tags: int\n",
        "        Number of different tags in the dataset.\n",
        "    tags: torch.LongTensor\n",
        "        Target labels. (batch_size, sequence_length).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    masks: torch.ByteTensor\n",
        "        Mask-like sparse tensor indicating the target label.\n",
        "        (batch_size, sequence_length, num_tags).\n",
        "    \"\"\"\n",
        "    copy_tags = tags.clone()\n",
        "    no_annotation_idx = (copy_tags == UNLABELED_INDEX)\n",
        "    copy_tags[no_annotation_idx] = 0\n",
        "    masks = torch.zeros(\n",
        "        copy_tags.size(0),\n",
        "        copy_tags.size(1),\n",
        "        num_tags,\n",
        "        dtype=torch.uint8,\n",
        "        device=tags.device\n",
        "    )\n",
        "    masks.scatter_(2, copy_tags.unsqueeze(2), 1)\n",
        "    masks[no_annotation_idx] = 1    # (batch_size, sequence_length, num_tags)\n",
        "    return masks    # type: ignore"
      ],
      "metadata": {
        "id": "01qDwvLrfyIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The LogSumExp trick"
      ],
      "metadata": {
        "id": "FZAEiEbafmWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_sum_exp(tensor: torch.Tensor, dim=-1, keepdim=False):\n",
        "    \"\"\"Compute log sum exp a numerically stable way for the forward algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tensor: torch.Tensor\n",
        "        Input tensor.\n",
        "    dim: int\n",
        "        Output dimension. Default is -1 for automatique determination.\n",
        "    keepdim: bool\n",
        "        If the output dimension shall be the same as the input's.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    max_score: torch.Tensor\n",
        "        Rank 0 tensor containing the highest scores of the input.\n",
        "    \"\"\"\n",
        "    max_score, _ = tensor.max(dim, keepdim=keepdim)\n",
        "    if keepdim:\n",
        "        stable_vec = tensor - max_score\n",
        "    else:\n",
        "        stable_vec = tensor - max_score.unsqueeze(dim)\n",
        "    return max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()"
      ],
      "metadata": {
        "id": "xKZAUH5FfpiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\Explain the `__init__` function, what's the `transitions` variable?"
      ],
      "metadata": {
        "id": "BLKyG662f5zV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPqrQIvLP_lL"
      },
      "outputs": [],
      "source": [
        "from abc import abstractmethod\n",
        "from typing import Optional, Literal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class BaseCRF(nn.Module):\n",
        "    \"\"\"Abstract method for the conditional random field (CRF) [1]_ .\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_tags: int\n",
        "        Number of possible tags (counting the padding one if needed).\n",
        "    padding_idx: int, optional\n",
        "        Padding index.\n",
        "    device: str, {\"cpu\", \"cuda\"}\n",
        "        Wether to do computation on GPU or CPU.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    num_tags: int\n",
        "        Number of possible tags (counting the padding one if needed).\n",
        "    start_transitions: torch.nn.Parameter\n",
        "        Begining scores of the transition matrix. Initialized with values\n",
        "        values sampled from a uniform distribution in [-1; 1]. (num_tags).\n",
        "    device: str, {\"cpu\", \"cuda\"}\n",
        "        Wether to do computation on GPU or CPU.\n",
        "    end_transitions: torch.nn.Parameter\n",
        "        Ending scores of the transition matrix. Initialized with values\n",
        "        values sampled from a uniform distribution in [-1; 1]. (num_tags).\n",
        "    transitions: torch.nn.Parameter\n",
        "        Transition matrix. Initialized using xavier [2]_'s method. Values are\n",
        "        sampled from a uniform distribution in [-1; 1]. (num_tags, num_tags).\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    ..  [1] Lafferty, John, Andrew McCallum, and Fernando CN Pereira.\n",
        "            \"Conditional random fields: Probabilistic models for segmenting and\n",
        "            labeling sequence data.\" (2001).\n",
        "    ..  [2] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
        "            training deep feedforward neural networks.\" Proceedings of the\n",
        "            thirteenth international conference on artificial intelligence and\n",
        "            statistics. JMLR Workshop and Conference Proceedings, 2010.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, num_tags: int, device: Literal[\"cpu\", \"cuda\"],\n",
        "        padding_idx: Optional[int]=None\n",
        "    ):\n",
        "        super(BaseCRF, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_tags = num_tags\n",
        "        self.start_transitions = nn.Parameter(\n",
        "            nn.init.uniform_(\n",
        "                torch.empty(num_tags, device=self.device), -1., 1.\n",
        "            )\n",
        "        )\n",
        "        self.end_transitions = nn.Parameter(\n",
        "            nn.init.uniform_(\n",
        "                torch.randn(num_tags, device=self.device), -1., 1.\n",
        "            )\n",
        "        )\n",
        "        init_transition = torch.empty(num_tags, num_tags, device=self.device)\n",
        "        if padding_idx is not None:\n",
        "            init_transition[:, padding_idx] = IMPOSSIBLE_SCORE\n",
        "            init_transition[padding_idx, :] = IMPOSSIBLE_SCORE\n",
        "        self.transitions = nn.Parameter(\n",
        "            nn.init.xavier_uniform_(init_transition)\n",
        "        )\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(\n",
        "        self, emissions: torch.FloatTensor, tags: torch.LongTensor,\n",
        "        mask: Optional[torch.ByteTensor]=None\n",
        "    ):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _forward_algorithm(\n",
        "        self, emissions: torch.FloatTensor, mask: torch.ByteTensor,\n",
        "        reverse_direction: bool=False\n",
        "    ):\n",
        "        \"\"\"Computes the logarithm of the unary/emission scores of each token\n",
        "        plus their transition score. Despite its name, this function is used to\n",
        "        compute the `forward-backward algorithm https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm`__.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emissions: torch.FloatTensor\n",
        "            Unary/emission score of each tokens.\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "        mask: torch.ByteTensor\n",
        "            Masked used to to discard subwords, special tokens or padding from\n",
        "            being added to the log-probability. (batch_size, sequence_length).\n",
        "        reverse_direction: bool, default=False\n",
        "            ``True`` if you want to use the backward algorithm. ``False`` if\n",
        "            you want to use the forward algorithm.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            Log-scores for each token. (sequence_length, batch_size, num_tags).\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
        "        broadcast_emissions = \\\n",
        "            emissions.transpose(0, 1).unsqueeze(2).contiguous()             # (sequence_length, batch_size, 1, num_tags)\n",
        "        mask = mask.float().transpose(0, 1).contiguous()                    # (sequence_length, batch_size) # type: ignore\n",
        "        broadcast_transitions = self.transitions.unsqueeze(0)               # (1, num_tags, num_tags)\n",
        "        sequence_iter = range(1, sequence_length)\n",
        "        # backward algorithm\n",
        "        if reverse_direction:\n",
        "            # Transpose transitions matrix and emissions\n",
        "            broadcast_transitions = broadcast_transitions.transpose(1, 2)   # (1, num_tags, num_tags)\n",
        "            broadcast_emissions = broadcast_emissions.transpose(2, 3)       # (sequence_length, batch_size, num_tags, 1)\n",
        "            sequence_iter = reversed(sequence_iter)\n",
        "            # It is beta\n",
        "            log_proba = [self.end_transitions.expand(batch_size, num_tags)] # [(batch_size, num_tags)]\n",
        "        # forward algorithm\n",
        "        else:\n",
        "            # It is alpha\n",
        "            log_proba = [                                                   # [(batch_size, num_tags)]\n",
        "                emissions.transpose(0, 1).contiguous()[0]\n",
        "                + self.start_transitions.unsqueeze(0)\n",
        "            ]\n",
        "        for i in sequence_iter:\n",
        "            # Broadcast log probability\n",
        "            broadcast_log_proba = log_proba[-1].unsqueeze(2)                # (batch_size, num_tags, 1)\n",
        "            # Add all scores\n",
        "            # inner: (batch_size, num_tags, num_tags)\n",
        "            # broadcast_log_proba:   (batch_size, num_tags, 1)\n",
        "            # broadcast_transitions: (1, num_tags, num_tags)\n",
        "            # broadcast_emissions:   (batch_size, 1, num_tags)\n",
        "            inner = (\n",
        "                broadcast_log_proba\n",
        "                + broadcast_transitions\n",
        "                + broadcast_emissions[i]\n",
        "            )\n",
        "            # Append log proba\n",
        "            log_proba.append(\n",
        "                torch.logsumexp(inner, dim=1) * mask[i].unsqueeze(1)\n",
        "                + log_proba[-1] * (1 - mask[i]).unsqueeze(1)\n",
        "            )\n",
        "        if reverse_direction:\n",
        "            log_proba.reverse()\n",
        "        return torch.stack(log_proba)                                       # type: ignore\n",
        "\n",
        "    def marginal_probabilities(\n",
        "        self, emissions: torch.FloatTensor,\n",
        "        mask: Optional[torch.ByteTensor]=None\n",
        "    ):\n",
        "        \"\"\"Computes the marginal probability of each token to belong to a given\n",
        "        class.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emissions: torch.FloatTensor\n",
        "            Unary/emission score of each tokens.\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "        mask: torch.ByteTensor, optional\n",
        "            Masked used to to discard subwords, special tokens or padding from\n",
        "            being added to the log-probability. (batch_size, sequence_length).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            Marginal probability of each token to belong to a given class.\n",
        "            (sequence_length, sequence_length, num_tags).\n",
        "        \"\"\"\n",
        "        if mask is None:\n",
        "            batch_size, sequence_length, _ = emissions.data.shape\n",
        "            mask = torch.ones(                  # type: ignore\n",
        "                [batch_size, sequence_length],\n",
        "                dtype=torch.uint8,\n",
        "                device=self.device\n",
        "            )\n",
        "        alpha = self._forward_algorithm(        # (sequence_length, batch_size, num_tags)\n",
        "            emissions,\n",
        "            mask,                               # type: ignore\n",
        "            reverse_direction=False\n",
        "        )\n",
        "        beta = self._forward_algorithm(         # (sequence_length, batch_size, num_tags)\n",
        "            emissions,\n",
        "            mask,                               # type: ignore\n",
        "            reverse_direction=True\n",
        "        )\n",
        "        z = torch.logsumexp(                    # (batch_size)\n",
        "            alpha[alpha.size(0) - 1] + self.end_transitions,\n",
        "            dim=1\n",
        "        )\n",
        "        proba = alpha + beta - z.view(1, -1, 1) # (sequence_length, batch_size, num_tags)\n",
        "        return torch.exp(proba)                 # (sequence_length, batch_size, num_tags) # type: ignore\n",
        "\n",
        "    def viterbi_decode(\n",
        "        self, emissions: torch.Tensor, mask: Optional[torch.ByteTensor]=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Dynamically computes the best sequence of tags.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emissions: torch.FloatTensor\n",
        "            Unary/emission score of each tokens.\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "        mask: torch.ByteTensor, optional\n",
        "            Masked used to to discard subwords, special tokens or padding from\n",
        "            being added to the log-probability. (batch_size, sequence_length).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        best_tags_list: List[int]\n",
        "            Best sequence of tag for each sequence in the batch.\n",
        "            (batch_size, ``torch.where(mask.shape[i]==1)``).\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = emissions.shape\n",
        "        if mask is None:\n",
        "            mask = torch.ones(                                                  # type: ignore\n",
        "                [batch_size, sequence_length],\n",
        "                dtype=torch.uint8,\n",
        "                device=self.device\n",
        "            )\n",
        "        emissions = emissions.transpose(0, 1).contiguous()\n",
        "        mask = mask.transpose(0, 1).contiguous()                                # type: ignore\n",
        "        # Start transition and first emission score\n",
        "        score = self.start_transitions + emissions[0]\n",
        "        history = []\n",
        "        for i in range(1, sequence_length):\n",
        "            broadcast_score = score.unsqueeze(2)\n",
        "            broadcast_emissions = emissions[i].unsqueeze(1)\n",
        "            next_score = \\\n",
        "                broadcast_score + self.transitions + broadcast_emissions\n",
        "            next_score, indices = next_score.max(dim=1)\n",
        "            score = torch.where(mask[i].unsqueeze(1) == 1, next_score, score)   # type: ignore\n",
        "            history.append(indices)\n",
        "        # Add end transition score\n",
        "        score += self.end_transitions\n",
        "        # Compute the best path\n",
        "        seq_ends = mask.sum(dim=0) - 1                                          # type: ignore\n",
        "        best_tags_list = []\n",
        "        for i in range(batch_size):\n",
        "            _, best_last_tag = score[i].max(dim=0)\n",
        "            best_tags = [best_last_tag.item()]\n",
        "            for hist in reversed(history[:seq_ends[i]]):\n",
        "                best_last_tag = hist[i][best_tags[-1]]\n",
        "                best_tags.append(best_last_tag.item())\n",
        "            best_tags.reverse()\n",
        "            best_tags_list.append(best_tags)\n",
        "        return best_tags_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This CRF layer is a bit modified as it is fuzzy but the logic remains the same."
      ],
      "metadata": {
        "id": "R0Vl5Ie-gufO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Literal\n",
        "\n",
        "\n",
        "class PartialCRF(BaseCRF):\n",
        "    \"\"\"Partial/Fuzzy Conditional random field.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_tags: int, device: Literal[\"cpu\", \"cuda\"],\n",
        "        padding_idx: Optional[int]=None\n",
        "    ):\n",
        "        super().__init__(num_tags, device, padding_idx)\n",
        "\n",
        "    def _numerator_score(\n",
        "        self, emissions: torch.FloatTensor, mask: torch.ByteTensor,\n",
        "        possible_tags: torch.ByteTensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes the log of the emission/unary score plus the transition score\n",
        "        for the whole sequence.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emissions: torch.FloatTensor\n",
        "            Unary/emission score of each tokens.\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "        mask: torch.ByteTensor\n",
        "            Masked used to to discard subwords, special tokens or padding from\n",
        "            being added to the log-probability. (batch_size, sequence_length).\n",
        "        possible_tags: torch.ByteTensor\n",
        "            Mask corresponding to the target label(s).\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            Log probability of the emission/unary score plus the transition\n",
        "            score for the whole sequence. (batch_size,)\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, num_tags = emissions.data.shape\n",
        "        emissions = emissions.transpose(0, 1).contiguous()                  # type: ignore\n",
        "        mask = mask.float().transpose(0, 1).contiguous()                    # type: ignore\n",
        "        possible_tags = possible_tags.float().transpose(0, 1)               # type: ignore\n",
        "\n",
        "        # Start transition score and first emission\n",
        "        first_possible_tag = possible_tags[0]\n",
        "        alpha = self.start_transitions + emissions[0]                       # (batch_size, num_tags)\n",
        "        alpha[(first_possible_tag == 0)] = IMPOSSIBLE_SCORE\n",
        "\n",
        "        for i in range(1, sequence_length):\n",
        "            current_possible_tags = possible_tags[i-1]                      # (batch_size, num_tags)\n",
        "            next_possible_tags = possible_tags[i]                           # (batch_size, num_tags)\n",
        "\n",
        "            # Emissions scores\n",
        "            emissions_score = emissions[i]\n",
        "            emissions_score[(next_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
        "            emissions_score = emissions_score.view(batch_size, 1, num_tags)\n",
        "\n",
        "            # Transition scores\n",
        "            transition_scores = self.transitions.unsqueeze(0).expand(\n",
        "                batch_size, num_tags, num_tags\n",
        "            ).clone()\n",
        "            transition_scores[(current_possible_tags == 0)] = IMPOSSIBLE_SCORE\n",
        "            transition_scores.transpose(1, 2)[(next_possible_tags == 0)] = \\\n",
        "                IMPOSSIBLE_SCORE\n",
        "\n",
        "            # Broadcast alpha\n",
        "            broadcast_alpha = alpha.unsqueeze(2)\n",
        "\n",
        "            # Add all scores\n",
        "            inner = broadcast_alpha + emissions_score + transition_scores   # (batch_size, num_tags, num_tags)\n",
        "            alpha = (\n",
        "                torch.logsumexp(inner, 1) * mask[i].unsqueeze(1)\n",
        "                + alpha * (1 - mask[i]).unsqueeze(1)\n",
        "            )\n",
        "\n",
        "        # Add end transition score\n",
        "        last_tag_indexes = mask.sum(0).long() - 1\n",
        "        end_transitions = (\n",
        "            self.end_transitions.expand(batch_size, num_tags)\n",
        "            * possible_tags.transpose(0, 1).view(\n",
        "                sequence_length * batch_size, num_tags\n",
        "            )[\n",
        "                last_tag_indexes\n",
        "                + torch.arange(batch_size, device=possible_tags.device)\n",
        "                * sequence_length\n",
        "            ]\n",
        "        )\n",
        "        end_transitions[(end_transitions == 0)] = IMPOSSIBLE_SCORE\n",
        "        stops = alpha + end_transitions                                     # (batch_size, num_tags)\n",
        "        return torch.logsumexp(stops, 1)                                    # (batch_size,) # type: ignore\n",
        "\n",
        "    def _denominator_score(\n",
        "        self, emissions: torch.FloatTensor, mask: torch.ByteTensor,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Computes the log-partition score for the whole sequence.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emissions: torch.FloatTensor\n",
        "            Unary/emission score of each tokens.\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "        mask: torch.ByteTensor\n",
        "            Masked used to to discard subwords, special tokens or padding from\n",
        "            being added to the log-probability. (batch_size, sequence_length).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            Log-partition score. (batch_size,)\n",
        "        \"\"\"\n",
        "        _, sequence_length, num_tags = emissions.data.shape\n",
        "        emissions = emissions.transpose(0, 1).contiguous()                  # type: ignore\n",
        "        mask = mask.float().transpose(0, 1).contiguous()                    # type: ignore\n",
        "        # Start transition score and first emissions score\n",
        "        alpha = self.start_transitions.view(1, num_tags) + emissions[0]\n",
        "\n",
        "        for i in range(1, sequence_length):\n",
        "            emissions_score = emissions[i].unsqueeze(1)                     # (batch_size, 1, num_tags)\n",
        "            transition_scores = self.transitions.unsqueeze(0)               # (1, num_tags, num_tags)\n",
        "            broadcast_alpha = alpha.unsqueeze(2)                            # (batch_size, num_tags, 1)\n",
        "            inner = broadcast_alpha + emissions_score + transition_scores   # (batch_size, num_tags, num_tags)\n",
        "            alpha = (\n",
        "                torch.logsumexp(inner, 1) * mask[i].unsqueeze(1)\n",
        "                + alpha * (1 - mask[i]).unsqueeze(1)\n",
        "            )\n",
        "\n",
        "        # Add end transition score\n",
        "        stops = alpha + self.end_transitions.unsqueeze(0)\n",
        "        return torch.logsumexp(stops, 1)                                    # (batch_size,) # type: ignore\n",
        "\n",
        "    def forward(\n",
        "        self, emissions: torch.FloatTensor, tags: torch.LongTensor,\n",
        "        mask: Optional[torch.ByteTensor]=None\n",
        "    ):\n",
        "        \"\"\"Performs the forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        emissions: torch.FloatTensor\n",
        "            Unary/emission score of each tokens.\n",
        "            (batch_size, sequence_length, num_tags).\n",
        "        tags: torch.LongTensor\n",
        "            Tensor containing the target labels. (batch_size, sequence_length).\n",
        "        mask: torch.ByteTensor, optional\n",
        "            Masked used to to discard subwords, special tokens or padding from\n",
        "            being added to the log-probability. (batch_size, sequence_length).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            Mean of the losses over the mini-batch. (0,)\n",
        "        \"\"\"\n",
        "        possible_tags = create_possible_tag_masks(self.num_tags, tags)      # (batch_size, sequence_length, num_tags)\n",
        "        gold_score = self._numerator_score(emissions, mask, possible_tags)  # (batch_size,) # type: ignore\n",
        "        forward_score = self._denominator_score(emissions, mask)            # (batch_size,) # type: ignore\n",
        "        nll = forward_score - gold_score                                    # (batch_size,)\n",
        "        return torch.mean(nll)                                              # Mean instead of sum # type: ignore"
      ],
      "metadata": {
        "id": "wT7dM7wWgtQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "sequence_length = 9\n",
        "num_tags = 5\n",
        "emissions = torch.randn(batch_size, sequence_length, num_tags)\n",
        "emissions"
      ],
      "metadata": {
        "id": "_N288qTJh5Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = torch.randint(0, 5, (batch_size, sequence_length))\n",
        "tags"
      ],
      "metadata": {
        "id": "dtCn7BGRh-pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.bernoulli(torch.empty(batch_size, sequence_length).uniform_(0, 1)).byte()\n",
        "mask"
      ],
      "metadata": {
        "id": "fpTGChP_iCXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the best sequence of tags for the emissions scores above."
      ],
      "metadata": {
        "id": "Dk-3r0J0m2p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf = PartialCRF(num_tags, device=\"cpu\")\n",
        "# Complete"
      ],
      "metadata": {
        "id": "rfVf5lQJiJ0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain the output, what is each dimension and what mean the numbers inside?"
      ],
      "metadata": {
        "id": "lyGjtJjUibUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crf.marginal_probabilities(emissions, mask)"
      ],
      "metadata": {
        "id": "HIYYenM8iZVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the loss of the emission scores obtained above."
      ],
      "metadata": {
        "id": "NgG8CVOcil-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete"
      ],
      "metadata": {
        "id": "4FPbNfjBikTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER Model"
      ],
      "metadata": {
        "id": "OWvOrxr6iqAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to build a class to feed the NER model with the data during training."
      ],
      "metadata": {
        "id": "dnUBbHf8j3RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from typing import Literal, List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, RobertaForMaskedLM\n",
        "\n",
        "\n",
        "class PartialNERDataset(Dataset):\n",
        "    \"\"\"Custom Dataset used to train the partial NER.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    device: str, {\"cpu\", \"cuda\"}\n",
        "        Deveice where the computations are performed.\n",
        "    max_length: int\n",
        "        Maximum sequence length.\n",
        "    iterable_corpus: List[List[str]]\n",
        "        Corpus containing lists of segmented texts.\n",
        "    labels: List[str]\n",
        "        List of possible labels.\n",
        "    iterable_labels: List[List[str]]\n",
        "        Corpus containing lists of labels mapped to the text at the same index\n",
        "        in ``iterable_corpus``.\n",
        "    lm_path: str\n",
        "        Path to a **transformers** pre-trained language model.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    device: str, {\"cpu\", \"cuda\"}\n",
        "        Deveice where the computations are performed.\n",
        "    max_length: int\n",
        "        Maximum sequence length.\n",
        "    iterable_corpus: List[List[str]]\n",
        "        Corpus containing lists of segmented texts.\n",
        "    iterable_labels: List[List[str]]\n",
        "        Corpus containing lists of labels mapped to the text at the same index\n",
        "        in ``iterable_corpus``.\n",
        "    label2idx: Dict[str, int]\n",
        "        Maps the string label to a unique integer id. Also adds a mapping for\n",
        "        the \"UNK\" label to -1.\n",
        "    tokenizer: AutoTokenizer\n",
        "        \"roberta-base\" tokenizer from **transformers**.\n",
        "    lm: RobertaForMaskedLM\n",
        "        Pre-trained RoBERTa used to perform language augmentation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, device: Literal[\"cpu\", \"cuda\"],\n",
        "        max_length: int, iterable_corpus: List[List[str]], labels: List[str],\n",
        "        iterable_labels: List[List[str]], lm_path: str\n",
        "    ):\n",
        "        self.iterable_corpus = iterable_corpus\n",
        "        self.iterable_labels = iterable_labels\n",
        "        self.label2idx = {label: idx for idx, label in enumerate(labels)}\n",
        "        self.label2idx[\"B-UNK\"] = -1\n",
        "        self.label2idx[\"I-UNK\"] = -1\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"roberta-base\", add_prefix_space=True\n",
        "        )\n",
        "        self.lm = RobertaForMaskedLM.from_pretrained(lm_path).to(device)\n",
        "        self.max_length = max_length\n",
        "        self.device = device\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.tokenize(idx)\n",
        "        y = torch.tensor(\n",
        "            self.align_labels(x, self.iterable_labels[idx]),\n",
        "            dtype=torch.int64,\n",
        "            device=self.device\n",
        "        )\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.iterable_corpus)\n",
        "\n",
        "    def tokenize(self, idx: int):\n",
        "        \"\"\"Tokenizes a segmented text from ``self.iterable_corpus`` at a given\n",
        "        index.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: int\n",
        "            Index of the segmented text to tokenize.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        inputs: transformers.BatchEncoding\n",
        "            Tokenized text.\n",
        "        \"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            self.iterable_corpus[idx],\n",
        "            is_split_into_words=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return inputs.to(self.device)\n",
        "\n",
        "    def align_labels(\n",
        "        self, inputs: transformers.BatchEncoding, labels: List[str]\n",
        "    ):\n",
        "        \"\"\"Align the sub-words with labels. All the sub-words are given the\n",
        "        the label of the original word. The padding token are given an \"O\"\n",
        "        label.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs: transformers.BatchEncoding\n",
        "            Tokenized text.\n",
        "        labels: List[str]\n",
        "            Word-level labels of the original text.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        label_ids: List[int]\n",
        "            Token-level labels of the tokenized text.\n",
        "        \"\"\"\n",
        "        word_ids = inputs.word_ids()    # type: ignore\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(self.label2idx[\"O\"])\n",
        "            else:\n",
        "                label_ids.append(self.label2idx[labels[word_idx]])\n",
        "        return label_ids\n"
      ],
      "metadata": {
        "id": "xfBTUBGSj9Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will plug the CRF layer on top of the language model in order to perform NER."
      ],
      "metadata": {
        "id": "BgVPZEhyir0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import Optional, Literal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import RobertaModel\n",
        "\n",
        "\n",
        "class PartialNER(nn.Module):\n",
        "    \"\"\"Partial Named Entity Recognizer (NER) Model.\n",
        "\n",
        "    This class defines a Partial NER model for named entity recognition tasks.\n",
        "    It extends the PyTorch ``nn.Module`` class and integrates with the\n",
        "    **Hugging Face** ``transformers`` library for handling pre-trained language\n",
        "    models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lm_path: str\n",
        "        The path or identifier of a pre-trained language model checkpoint.\n",
        "    num_labels: int\n",
        "        The number of unique labels or tags for NER.\n",
        "    device: str, {\"cpu\", \"cuda\"}\n",
        "        The device on which the model will be instantiated (\"cpu\" or \"cuda\").\n",
        "    dropout: float\n",
        "        The dropout probability to apply to the model's hidden states.\n",
        "    padding_idx : Optional[int], optional\n",
        "        The padding index for the input sequences. If None, the default index\n",
        "        is used.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    device: str\n",
        "        The device on which the model is instantiated.\n",
        "    transformer: transformers.RobertaModel\n",
        "        The pre-trained transformer model used for feature extraction.\n",
        "    linear_dropout: nn.Dropout\n",
        "        The dropout layer applied to the model's linear layer.\n",
        "    fc: nn.Linear\n",
        "        The linear layer mapping features to label scores.\n",
        "    crf: PartialCRF\n",
        "        The partial conditional random field layer for structured prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, lm_path: str, num_labels: int, device: Literal[\"cpu\", \"cuda\"],\n",
        "        dropout: float, padding_idx: Optional[int]=None\n",
        "    ):\n",
        "        super(PartialNER, self).__init__()\n",
        "        self.device = device\n",
        "        logging.info(f\"Loading LM checkpoint from {lm_path}\")\n",
        "        self.transformer = RobertaModel.from_pretrained(lm_path)\n",
        "        self.linear_dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(768, num_labels)    # (batch_size, max_length, num_labels) # Complete with the correct input dimensioon\n",
        "        self.crf = PartialCRF(\n",
        "            num_tags=num_labels,\n",
        "            device=device,\n",
        "            padding_idx=padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, inputs: transformers.BatchEncoding,\n",
        "        # inputs_augmented: transformers.BatchEncoding,\n",
        "        outputs: torch.LongTensor,\n",
        "    ):\n",
        "        \"\"\"Performs the forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs: transformer.BatchEncoding\n",
        "            Original sentence, tokenized with ``transformers``.\n",
        "        inputs_augmented: torch.BatchEncoding\n",
        "            Language augmented input, tokenized with ``transformers``.\n",
        "        outputs: torch.LongTensor\n",
        "            List of true labels.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.FloatTensor\n",
        "            Sum over the loss of the original input and the augmented input.\n",
        "        \"\"\"\n",
        "        h = self.transformer(**inputs).last_hidden_state\n",
        "        logits = self.fc(self.linear_dropout(h))# Complete: get the logits from the language model\n",
        "        loss = self.crf(\n",
        "            emissions=logits,\n",
        "            tags=outputs,\n",
        "            mask=inputs[\"attention_mask\"],\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def viterbi_decode(self, inputs: transformers.BatchEncoding):\n",
        "        \"\"\"Computes the mostly likely label sequence.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs: transformers.BatchEncoding\n",
        "            Input sentence tokenized with ``transformers``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        outputs: List[List[int]]\n",
        "            Most likely tag sequence of each input in the batch.\n",
        "        \"\"\"\n",
        "        h = self.transformer(**inputs).last_hidden_state\n",
        "        logits = self.fc(self.linear_dropout(h))\n",
        "        outputs = self.crf.viterbi_decode(\n",
        "            logits,\n",
        "            mask=inputs[\"attention_mask\"]\n",
        "        )\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "FFaEPs01irO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the `PartialNERDataset` class in order to pass training examples through the NER model. No need to train, you just need to output the loss at each pass in order to test your pipeline."
      ],
      "metadata": {
        "id": "7NojJkw9mVfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "def read_conll(path: str):\n",
        "    \"\"\"Reads a `conll` file and returns a tuple containing the list of tokens\n",
        "    per doc and tags epr doc.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path: str\n",
        "        Path to the conll file.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    token_docs: List[List[str]]\n",
        "        List of tokens per document.\n",
        "    tag_docs: List[List[str]]\n",
        "        List of labels per document.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_text = f.read().strip()\n",
        "\n",
        "    raw_docs = re.split(r\"\\n\\t?\\n\", raw_text)\n",
        "    token_docs = []\n",
        "    tag_docs = []\n",
        "    for doc in raw_docs:\n",
        "        tokens = []\n",
        "        tags = []\n",
        "        for line in doc.split(\"\\n\"):\n",
        "            token, tag = line.split(\"\\t\")\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "        token_docs.append(tokens)\n",
        "        tag_docs.append(tags)\n",
        "    return token_docs, tag_docs"
      ],
      "metadata": {
        "id": "dSsAJ5TOhcGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "iterable_corpus, iterable_labels = read_conll(\"./conll_train.conll\")\n",
        "\n",
        "with open(\"./labels.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  labels = f.read().splitlines()\n",
        "\n",
        "dataset = PartialNERDataset(\n",
        "    device=\"cpu\",\n",
        "    max_length=256,\n",
        "    iterable_corpus=iterable_corpus,\n",
        "    labels=labels,\n",
        "    iterable_labels=iterable_labels,\n",
        "    lm_path=\"roberta-base\"\n",
        ")\n",
        "\n",
        "tmp_dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "ner = PartialNER(\n",
        "    lm_path=\"roberta-base\",\n",
        "    num_labels=len(labels),\n",
        "    device=\"cpu\",\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "for x, y in tmp_dataloader:\n",
        "  print(x)\n",
        "  print(ner(x, y))"
      ],
      "metadata": {
        "id": "GLypG5mxmU6G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a1cac5b-07f1-4ab5-ffef-af2975214b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[[    0,   772,   439,    66,   321,     4,  2546,   715,   795,    23,\n",
            "           8301,     4,  2546,   479,     2,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1]],\n",
            "\n",
            "        [[    0,    22,  3709,  1051,    10,   319,     9,  8724,    14, 19815,\n",
            "             21,   164,     7,    28,     5,  1948,  2156,    22, 31139,  1371,\n",
            "             26,   479,    22,     2,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "              1,     1,     1,     1,     1,     1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "          0, 0, 0]]])}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a0f745791a69>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d4138ef4f7a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mSum\u001b[0m \u001b[0mover\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0maugmented\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# Complete: get the logits from the language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         loss = self.crf(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either input_ids or inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}