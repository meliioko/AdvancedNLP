{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Imports & Implementation\n",
        "\n",
        "Copy your implementation of GPT2 from Lab Session 3, and instantiate a model with the weights of GPT2."
      ],
      "metadata": {
        "id": "M091YWuTdKEh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w76ioKtadmH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2L1o7-TBSCD"
      },
      "source": [
        "## Part 1: KV caching\n",
        "\n",
        "To make our model faster, we implement KV caching in this section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXAdrD-aBgjZ"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Re-implement the `head_level_self_attention` function so it can include a KV cache. Careful: Q, K and V should now correspond only to inputs that are not in the cache. This function should return the attention output and the updated cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIEHlOaHBgjq"
      },
      "outputs": [],
      "source": [
        "def head_level_self_attention(Q, K, V, causal=True, cached_kv=None):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqSt0HiXHpqN"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Implement the `CustomTransformerWithCache` inheriting from `CustomTransformer`, with a forward function that takes `cached_kv` as an argument, and returns the updated KV cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXpoWc4rHpqP"
      },
      "outputs": [],
      "source": [
        "class CustomTransformerWithCache(CustomTransformer):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2F2vsyzIjf8"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "Create the `CustomGpt2WithCache` class using the `CustomTransformerWithCache` block. Instantiate a `CustomGpt2WithCache` object with the weights of the original GPT-2. The forward will return the KV caches of each Transformer layer in a tuple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pfzl5MiIjgE"
      },
      "outputs": [],
      "source": [
        "class CustomGpt2WithCache(nn.Module):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXSR_YwtJwF-"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Test the KV cache behaviour by simulating two steps of greedy generation with the cache system:\n",
        "- Forward a whole sequence and keep the KV cache (step 1). Add the next predicted token to the sequence.\n",
        "- Feed the new sequence **and the KV cache** to your GPT-2.\n",
        "\n",
        "Compare the resulting prediction with and without cache, and check that they are similar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkufXiDIJwF_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1zb3NqwLMzo"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "Implement the greedy generation function with KV caching. Compare it to the vanilla greedy generation without cache with `%timeit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymLpGANlLMz2"
      },
      "outputs": [],
      "source": [
        "def greedy_generate_with_cache(model, sentence, ...):\n",
        "  ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzc4fnReLzPf"
      },
      "source": [
        "# Part 2 : Streaming LLM\n",
        "\n",
        "The paper [Efficient Streaming Language Models with Attention Sinks\n",
        "](https://arxiv.org/pdf/2309.17453.pdf) proposes a KV caching method that allows model to generate beyond their context window with minimal performance loss. Implement their approach in your KV cache system and measure the resulting performance gaps."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mf_zk3LIexB7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}