{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Course 3 : Language Modeling\n",
        "\n",
        "The slides of the course are available [here](https://github.com/NathanGodey/AdvancedNLP/raw/main/slides/pdf/course3_lm.pdf)"
      ],
      "metadata": {
        "id": "hnwkEOsOHJqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Homemade Transformers\n",
        "\n",
        "In this section, we will reproduce the forward pass of a Transformers from scratch. **Don't forget to enable the GPU.**"
      ],
      "metadata": {
        "id": "Q2iTpErOJ_Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "MkFeXnXswDNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "Given a Query, Key or Value tensor of shape `batch_size x sequence_length x hidden_dim`, design a function (in PyTorch) that adds a head dimension for `num_heads` heads."
      ],
      "metadata": {
        "id": "hR3ZJ11NvLNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_heads(input_tensor, num_heads):\n",
        "  ..."
      ],
      "metadata": {
        "id": "UAUHNPbUvJxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_Q = torch.randn(8, 128, 64)\n",
        "assert split_into_heads(test_Q, 8).shape == (8, 128, 8, 8)"
      ],
      "metadata": {
        "id": "W1TpGHwmwCAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "Given a split Query, Key or Value tensor of shape `batch_size x sequence_length x num_heads x head_hidden_dim`, design the function (in PyTorch) that removes the head dimension."
      ],
      "metadata": {
        "id": "aQVU43kuwfjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_heads(input_tensor):\n",
        "  ..."
      ],
      "metadata": {
        "id": "gXLhjErGwtzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concat_heads(split_into_heads(test_Q, 8)) - test_Q"
      ],
      "metadata": {
        "id": "BUs7YMmmwzpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3\n",
        "Given a Query, Key and Value tensors of shape `batch_size x sequence_length x num_heads x head_hidden_dim`, design the function that performs the self-attention product. Test it with random inputs."
      ],
      "metadata": {
        "id": "DtSFffHKw89e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def head_level_self_attention(Q, K, V):\n",
        "  ..."
      ],
      "metadata": {
        "id": "C3vwf__GxY6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4\n",
        "Rewrite the function from Question 3 allowing the use of a causal mask:"
      ],
      "metadata": {
        "id": "IOvuKF4uxiTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def head_level_self_attention(Q, K, V, causal=True):\n",
        "  ..."
      ],
      "metadata": {
        "id": "uYWq4YKZ1zYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5\n",
        "Create a `CustomTransformer` class (`nn.Module` child) using the previous functions. The forward pass will go as:\n",
        "1. Use a first LayerNorm\n",
        "2. Compute (Q, K, V) with a single linear projection `hidden_dim -> 3*hidden_dim` (with bias)\n",
        "3. Compute self-attention\n",
        "4. Do a linear projection keeping dimension (with bias)\n",
        "5. Add original input to current result\n",
        "6. Use a second LayerNorm\n",
        "7. Do a linear projection (with bias) to some `intermediate_dim`\n",
        "8. Apply a given activation function (argument of the class)\n",
        "9. Do a linear projection (with bias) back to `hidden_dim``\n",
        "10. Add output of step 5 to current result\n"
      ],
      "metadata": {
        "id": "gUGf3yni18q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransformer(nn.Module):\n",
        "  ..."
      ],
      "metadata": {
        "id": "rt6C4EOt3tya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6\n",
        "\n",
        "Create a `CustomInputEmbedding` class (`nn.Module` child) that generates input embeddings from batched input tokens ids. It will provide one token embedding for each input token and add an absolute positional embedding."
      ],
      "metadata": {
        "id": "IMxcTQ9M3xmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomInputEmbedding(nn.Module):\n",
        "  ..."
      ],
      "metadata": {
        "id": "RL17E9cQ4VJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7\n",
        "\n",
        "The GPT-2 model family was designed the following way:\n",
        "- Embed input tokens adding an absolute positional embedding\n",
        "- Pass through $N$ Transformer layers\n",
        "- Apply a final LayerNorm\n",
        "- Use a Linear LM head to make a prediction\n",
        "\n",
        "Using all previous classes, create a `CustomGpt2` module. Test it on random inputs."
      ],
      "metadata": {
        "id": "8gY7gaeJ4duH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGpt2(nn.Module):\n",
        "  ..."
      ],
      "metadata": {
        "id": "zExOMU5O5HwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Weight conversion\n",
        "\n",
        "In this section, we import the weights of the original GPT-2 (small version) and we convert them into our custom format."
      ],
      "metadata": {
        "id": "mV_CiXB95L-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7\n",
        "Download the `gpt2` model from HuggingFace as an `AutoModelForCausalLM`. Print it and find out its hyper-parameters. Instantiate a similar `CustomGpt2` model."
      ],
      "metadata": {
        "id": "7rqfnUSb5gWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wTGymyvH577I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 8\n",
        "Create a function that converts a `Conv1D` layer into a `nn.Linear` layer. Check if the Conv1D and its Linear counterpart give the same results on random inputs, and if they run as fast."
      ],
      "metadata": {
        "id": "GTyAm9IX598P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2linear(conv_layer):\n",
        "  ..."
      ],
      "metadata": {
        "id": "6tCz_BDe6KRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 9\n",
        "Create a `convert_weights` function that sets all equivalent parameters in your `CustomGpt2` model to the values of their HuggingFace counterpart. Make a real-life prediction to check that their outputs are similar."
      ],
      "metadata": {
        "id": "PdSszYFt6Vtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_weights(original_gpt2, custom_gpt2):\n",
        "  ..."
      ],
      "metadata": {
        "id": "_oao4Kw860JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Generation\n",
        "\n",
        "Let's now use our model in generation mode."
      ],
      "metadata": {
        "id": "ZLLHg79O60yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 10\n",
        "\n",
        "Write a `greedy_generate` function that uses your custom GPT2 and performs greedy generation. Try it on a short sentence (don't forget a stopping condition)."
      ],
      "metadata": {
        "id": "YSawFDU_7fJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_generate(model, sentence, ...):\n",
        "  ..."
      ],
      "metadata": {
        "id": "7iRHVVXr7dmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"...\"\n",
        "tokens = greedy_generate(model, sentence, ...)\n",
        "print(tokenizer.decode(tokens))"
      ],
      "metadata": {
        "id": "jRyCFFdB88FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 11\n",
        "\n",
        "Write a `topk_generate` function that uses your custom GPT2 and performs top-k generation (sampling in top-k tokens)."
      ],
      "metadata": {
        "id": "D9cDBJ8s8pQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def topk_generate(model, sentence, k, ...):\n",
        "  ..."
      ],
      "metadata": {
        "id": "IMC-jfne8pQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 12\n",
        "\n",
        "Write a `nucleus_generate` function that uses your custom GPT2 and performs top-p generation (sampling in tokens until cumulated probability is greater than p)."
      ],
      "metadata": {
        "id": "uxZVI9cL-aM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nucleus_generate(model, sentence, p, ...):\n",
        "  ..."
      ],
      "metadata": {
        "id": "mWtuCBcI-aM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 13\n",
        "\n",
        "Write a `beam_generate` function that uses your custom GPT2 and performs beam-search generation."
      ],
      "metadata": {
        "id": "mpBija6c-rdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_generate(model, sentence, num_beams, ...):\n",
        "  ..."
      ],
      "metadata": {
        "id": "snn8uxEB-rdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 14\n",
        "\n",
        "Using the `%timeit` magic operation, measure and compare the throughput of each generation method."
      ],
      "metadata": {
        "id": "W4gFL2gPAy6H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nv0nMpk9Ay6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: KV caching\n",
        "\n",
        "To make our model faster, we implement KV caching in this section."
      ],
      "metadata": {
        "id": "O2L1o7-TBSCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 15\n",
        "\n",
        "Re-implement the `head_level_self_attention` function so it can include a KV cache. Careful: Q, K and V should now correspond only to inputs that are not in the cache. This function should return the attention output and the updated cache."
      ],
      "metadata": {
        "id": "pXAdrD-aBgjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def head_level_self_attention(Q, K, V, causal=True, cached_kv=None):\n",
        "  ..."
      ],
      "metadata": {
        "id": "lIEHlOaHBgjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 16\n",
        "\n",
        "Implement the `CustomTransformerWithCache` inheriting from `CustomTransformer`, with a forward function that takes `cached_kv` as an argument, and returns the updated KV cache."
      ],
      "metadata": {
        "id": "VqSt0HiXHpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransformerWithCache(CustomTransformer):\n",
        "  ..."
      ],
      "metadata": {
        "id": "yXpoWc4rHpqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 17\n",
        "\n",
        "Create the `CustomGpt2WithCache` class using the `CustomTransformerWithCache` block. Instantiate a `CustomGpt2WithCache` object with the weights of the original GPT-2. The forward will return the KV caches of each Transformer layer in a tuple."
      ],
      "metadata": {
        "id": "i2F2vsyzIjf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomGpt2WithCache(nn.Module):\n",
        "  ..."
      ],
      "metadata": {
        "id": "-pfzl5MiIjgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 18\n",
        "\n",
        "Test the KV cache behaviour by simulating two steps of greedy generation with the cache system:\n",
        "- Forward a whole sequence and keep the KV cache (step 1). Add the next predicted token to the sequence.\n",
        "- Feed the new sequence **and the KV cache** to your GPT-2.\n",
        "\n",
        "Compare the resulting prediction with and without cache, and check that they are similar."
      ],
      "metadata": {
        "id": "sXSR_YwtJwF-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkufXiDIJwF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 19\n",
        "\n",
        "Implement the greedy generation function with KV caching. Compare it to the vanilla greedy generation without cache with `%timeit`."
      ],
      "metadata": {
        "id": "d1zb3NqwLMzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_generate_with_cache(model, sentence, ...):\n",
        "  ..."
      ],
      "metadata": {
        "id": "ymLpGANlLMz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus : Streaming LLM\n",
        "\n",
        "The paper [Efficient Streaming Language Models with Attention Sinks\n",
        "](https://arxiv.org/pdf/2309.17453.pdf) proposes a KV caching method that allows model to generate beyond their context window with minimal performance loss. Implement their approach in your KV cache system and measure the resulting performance gaps."
      ],
      "metadata": {
        "id": "Yzc4fnReLzPf"
      }
    }
  ]
}